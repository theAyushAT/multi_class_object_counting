{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vehicle_count.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theAyushAT/multi_class_object_counting/blob/main/vehicle_count.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWlkwoIFXSHK"
      },
      "source": [
        "# Multi class object counting\n",
        "\n",
        "Ref repo : \n",
        "\n",
        "https://github.com/cfotache/pytorch_objectdetecttrack\n",
        "\n",
        "https://github.com/abewley/sort\n",
        "\n",
        "\n",
        "Input video link https://drive.google.com/file/d/1pLv-quRmgGPQu3i6fY11D_bxXkZtz1hD/view?usp=sharing\n",
        "\n",
        "\n",
        "Output Video Link Output video - https://drive.google.com/file/d/1areV8LB93m_VFaqPIVDyN1jlj3ubKY6Q/view?usp=sharing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy-xArNjBljV"
      },
      "source": [
        "# repo https://github.com/cfotache/pytorch_objectdetecttrack\n",
        "!curl --header 'Host: codeload.github.com' --user-agent 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0' --header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8' --header 'Accept-Language: en-US,en;q=0.5' --referer 'https://github.com/cfotache/pytorch_objectdetecttrack' --cookie '_octo=GH1.1.171669365.1609144657; logged_in=yes; dotcom_user=theAyushAT; color_mode=%7B%22color_mode%22%3A%22light%22%2C%22light_theme%22%3A%7B%22name%22%3A%22light%22%2C%22color_mode%22%3A%22light%22%7D%2C%22dark_theme%22%3A%7B%22name%22%3A%22dark%22%2C%22color_mode%22%3A%22dark%22%7D%7D; tz=Asia%2FKolkata' --header 'Upgrade-Insecure-Requests: 1' 'https://codeload.github.com/cfotache/pytorch_objectdetecttrack/zip/refs/heads/master' --output 'pytorch_objectdetecttrack-master.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzRTcqGdE8gS"
      },
      "source": [
        "!unzip /content/pytorch_objectdetecttrack-master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjyJujdbFEcC",
        "outputId": "78ec4790-df9a-4e94-d68f-2d95093bef32"
      },
      "source": [
        "%cd /content/pytorch_objectdetecttrack-master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pytorch_objectdetecttrack-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp66X1d5FsmR",
        "outputId": "a8b03048-ec6d-4559-9930-f239169e99ae"
      },
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-19 11:51:57--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M  71.7MB/s    in 3.5s    \n",
            "\n",
            "2021-06-19 11:52:01 (67.6 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB3n8dkl-fz0"
      },
      "source": [
        "!pip install filterpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6B3UhM4cC7D"
      },
      "source": [
        "video link https://drive.google.com/file/d/1pLv-quRmgGPQu3i6fY11D_bxXkZtz1hD/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2vDKo-5ubtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9fa3d03-b011-49f5-bfb7-fe73ebf24bfb"
      },
      "source": [
        "# video\n",
        "!curl --header 'Host: doc-10-b8-docs.googleusercontent.com' --user-agent 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0' --header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8' --header 'Accept-Language: en-US,en;q=0.5' --cookie 'AUTH_n1o21ebub997en8ucnes7jnsdk8r5ipt_nonce=7hjn88hlr00u6' --header 'Upgrade-Insecure-Requests: 1' 'https://doc-10-b8-docs.googleusercontent.com/docs/securesc/dnofsi9g1vmid9d0k9i7iuf4r9kpuvl9/a3isoodjja91tl2go6nv3kakb68373a9/1624104750000/09311022561292723842/09651870378833774979/1QUClOAWRVqNJII1crbCbdStJYzq9Uhfx?e=download&authuser=0&nonce=7hjn88hlr00u6&user=09651870378833774979&hash=8mhppmvrhhdk6asreck8gu4d0pasfa0h' --output 'video.mp4'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FFGWGFSZdG0"
      },
      "source": [
        "from models import *\n",
        "from utils import *\n",
        "\n",
        "import os, sys, time, datetime, random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOe_zJV5bZhi"
      },
      "source": [
        "For detction YOLOv3 with Darknet backbone is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t44_yeS8ZmIl"
      },
      "source": [
        "# load weights and set defaults\n",
        "config_path='config/yolov3.cfg'\n",
        "weights_path='yolov3.weights'\n",
        "class_path='config/coco.names'\n",
        "img_size=416\n",
        "conf_thres=0.8\n",
        "nms_thres=0.4\n",
        "\n",
        "# load model and put into eval mode\n",
        "model = Darknet(config_path, img_size=img_size)\n",
        "model.load_weights(weights_path)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "\n",
        "classes = utils.load_classes(class_path)\n",
        "Tensor = torch.cuda.FloatTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF1GXFwLZzUD"
      },
      "source": [
        "def detect_image(img):\n",
        "    # scale and pad image\n",
        "    ratio = min(img_size/img.size[0], img_size/img.size[1])\n",
        "    imw = round(img.size[0] * ratio)\n",
        "    imh = round(img.size[1] * ratio)\n",
        "    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
        "         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
        "                        (128,128,128)),\n",
        "         transforms.ToTensor(),\n",
        "         ])\n",
        "    # convert image to Tensor\n",
        "    image_tensor = img_transforms(img).float()\n",
        "    image_tensor = image_tensor.unsqueeze_(0)\n",
        "    input_img = Variable(image_tensor.type(Tensor))\n",
        "    # run inference on the model and get detections\n",
        "    with torch.no_grad():\n",
        "        detections = model(input_img)\n",
        "        detections = utils.non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
        "    return detections[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Elr7-LJb_II"
      },
      "source": [
        "Object tracking is performed using SORT.\n",
        "\n",
        "Ref Repo https://github.com/abewley/sort\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzcynPOMxohC",
        "outputId": "27d5cb0c-c06b-45c4-c69b-a7795987b92a"
      },
      "source": [
        "videopath = '/content/pytorch_objectdetecttrack-master/video.mp4'\n",
        "\n",
        "import cv2\n",
        "from sort import *\n",
        "colors=[(255,0,0),(0,255,0),(0,0,255),(255,0,255),(128,0,0),(0,128,0),(0,0,128),(128,0,128),(128,128,0),(0,128,128)]\n",
        "\n",
        "vid = cv2.VideoCapture(videopath)\n",
        "mot_tracker = Sort() \n",
        "\n",
        "\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "ret,frame=vid.read()\n",
        "vw = frame.shape[1]\n",
        "vh = frame.shape[0]\n",
        "print (\"Video size\", vw,vh)\n",
        "fps = vid.get(cv2.CAP_PROP_FPS) \n",
        "outvideo = cv2.VideoWriter(videopath.replace(\".mp4\", \"-det.mp4\"),fourcc,int(fps),(vw,vh))\n",
        "\n",
        "frames = 0\n",
        "cls_lst = []\n",
        "id_lst =set ()\n",
        "map_dict = {}\n",
        "starttime = time.time()\n",
        "while(True):\n",
        "    ret, frame = vid.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    # if frames % 2 ==0:\n",
        "    #     frames += 1\n",
        "    #     continue\n",
        "    frames += 1\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pilimg = Image.fromarray(frame)\n",
        "    detections = detect_image(pilimg)\n",
        "\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "    img = np.array(pilimg)\n",
        "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
        "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
        "    unpad_h = img_size - pad_y\n",
        "    unpad_w = img_size - pad_x\n",
        "    if detections is not None:\n",
        "        tracked_objects = mot_tracker.update(detections.cpu())\n",
        "\n",
        "        unique_labels = detections[:, -1].cpu().unique()\n",
        "        n_cls_preds = len(unique_labels)\n",
        "        for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
        "            box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
        "            box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
        "            y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
        "            x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
        "            color = colors[int(obj_id) % len(colors)]\n",
        "            # print(classes)\n",
        "            # print(int(cls_pred))\n",
        "            cls = classes[int(cls_pred)]\n",
        "            \n",
        "            if obj_id not in id_lst:\n",
        "              cls_lst.append(cls)\n",
        "              id_lst.add(obj_id)\n",
        "              count = cls_lst.count(cls)\n",
        "              map_dict.update({int(obj_id):int(count)})\n",
        "\n",
        "            \n",
        "            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 4)\n",
        "            cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+80, y1), color, -1)\n",
        "            cv2.putText(frame, str(cls) + \"-\" + str(map_dict[int(obj_id)]), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n",
        "\n",
        "    outvideo.write(frame)\n",
        "\n",
        "    if ch == 27:\n",
        "        break\n",
        "\n",
        "totaltime = time.time()-starttime\n",
        "print(frames, \"frames\", totaltime/frames, \"s/frame\")\n",
        "outvideo.release()\n",
        "\n",
        "print('number of cars are :',cls_lst.count('car'))\n",
        "print('number of motorbikes are :',cls_lst.count('motorbike'))\n",
        "print('number of buses are :',cls_lst.count('bus'))\n",
        "print('number of trucks are :',cls_lst.count('truck'))\n",
        "print('number of bicycles are :',cls_lst.count('bicycle'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Video size 1280 720\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5395 frames 0.06157193921913805 s/frame\n",
            "number of cars are : 544\n",
            "number of motorbikes are : 0\n",
            "number of buses are : 13\n",
            "number of trucks are : 110\n",
            "number of bicycles are : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myIfT_Klc-oi"
      },
      "source": [
        "Output video - https://drive.google.com/file/d/1areV8LB93m_VFaqPIVDyN1jlj3ubKY6Q/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvoRmHpXyKPF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}